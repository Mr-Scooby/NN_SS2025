{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: urllib3 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (1.26.13)\n",
      "Requirement already satisfied: fastprogress in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0) (2025.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch==2.2.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: torchvision==0.17.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (0.17.0)\n",
      "Requirement already satisfied: numpy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchvision==0.17.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchvision==0.17.0) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchvision==0.17.0) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchvision==0.17.0) (11.0.0)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchvision==0.17.0) (2025.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch==2.2.0->torchvision==0.17.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchvision==0.17.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchvision==0.17.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchvision==0.17.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchvision==0.17.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch==2.2.0->torchvision==0.17.0) (1.3.0)\n",
      "Requirement already satisfied: torchaudio==2.2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (2.2.0)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchaudio==2.2.0) (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchaudio==2.2.0) (2025.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch==2.2.0->torchaudio==2.2.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch==2.2.0->torchaudio==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: torchtext==0.16.2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (0.16.2)\n",
      "Requirement already satisfied: tqdm in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchtext==0.16.2) (4.67.1)\n",
      "Requirement already satisfied: requests in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchtext==0.16.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchtext==0.16.2) (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchtext==0.16.2) (1.26.4)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchtext==0.16.2) (0.7.1)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch==2.2.0->torchtext==0.16.2) (2025.5.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchdata==0.7.1->torchtext==0.16.2) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch==2.2.0->torchtext==0.16.2) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchtext==0.16.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchtext==0.16.2) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchtext==0.16.2) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch==2.2.0->torchtext==0.16.2) (1.3.0)\n",
      "Requirement already satisfied: torchdata==0.7.1 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchdata==0.7.1) (1.26.13)\n",
      "Requirement already satisfied: requests in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchdata==0.7.1) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchdata==0.7.1) (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=2->torchdata==0.7.1) (2025.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch>=2->torchdata==0.7.1) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchdata==0.7.1) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchdata==0.7.1) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from requests->torchdata==0.7.1) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
      "Requirement already satisfied: torchmetrics==0.11.4 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (0.11.4)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchmetrics==0.11.4) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.8.1 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchmetrics==0.11.4) (2.2.0)\n",
      "Requirement already satisfied: packaging in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torchmetrics==0.11.4) (25.0)\n",
      "Requirement already satisfied: filelock in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from torch>=1.8.1->torchmetrics==0.11.4) (2025.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from jinja2->torch>=1.8.1->torchmetrics==0.11.4) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/radek/opt/anaconda3/envs/NN_SS2025/lib/python3.12/site-packages (from sympy->torch>=1.8.1->torchmetrics==0.11.4) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# These commands will install the necessary packages and their dependencies in the current env.\n",
    "\n",
    "# uncomment the following line to install the packages in the current env\n",
    "\n",
    "!pip install numpy pandas urllib3 fastprogress\n",
    "!pip install torch==2.2.0 #+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchvision==0.17.0 #+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchaudio==2.2.0 #+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchtext==0.16.2 #+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchdata==0.7.1 #+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install torchmetrics==0.11.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to PyTorch and Autograd\n",
    "\n",
    "This Jupyter Notebook provides an introduction to PyTorch's autograd functionality and gradient computation. It covers various topics including:\n",
    "\n",
    "- Basic tensor operations and conversions between NumPy arrays and PyTorch tensors.\n",
    "- Mathematical operations using PyTorch tensors.\n",
    "- The concept of autograd and how to use it for automatic differentiation.\n",
    "- Computing gradients and higher-order derivatives.\n",
    "- Intermediate gradient computation using `retain_grad` and hooks.\n",
    "- Solving nonlinear equations using Newton-Raphson's method with PyTorch.\n",
    "- Gradient computation through nonlinear solvers.\n",
    "\n",
    "The notebook includes practical examples and code snippets to demonstrate these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch and Numpy\n",
    "\n",
    "Details about math operation in torch can be found in: http://pytorch.org/docs/torch.html#math-operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Numpy array:\n",
      " [[0 1 2]\n",
      " [3 4 5]] \n",
      "\n",
      " Torch tensor:\n",
      " tensor([[0, 1, 2],\n",
      "        [3, 4, 5]]) \n",
      "\n",
      " tensor to array:\n",
      " [[0 1 2]\n",
      " [3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "# convert numpy to tensor or vise versa\n",
    "np_data = np.arange(6).reshape((2, 3))\n",
    "torch_data = torch.from_numpy(np_data)\n",
    "tensor2array = torch_data.numpy()\n",
    "print(\n",
    "    '\\n Numpy array:\\n', np_data,          # [[0 1 2], [3 4 5]]\n",
    "    '\\n\\n Torch tensor:\\n', torch_data,      #  0  1  2 \\n 3  4  5    [torch.LongTensor of size 2x3]\n",
    "    '\\n\\n tensor to array:\\n', tensor2array, # [[0 1 2], [3 4 5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "abs \n",
      "numpy:  [1 2 1 2] \n",
      "torch:  tensor([1., 2., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "# abs\n",
    "data = [-1, -2, 1, 2]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "print(\n",
    "    '\\nabs',\n",
    "    '\\nnumpy: ', np.abs(data),          # [1 2 1 2]\n",
    "    '\\ntorch: ', torch.abs(tensor)      # [1 2 1 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 2.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sin function- \n",
      "numpy:\n",
      " [-0.84147098 -0.90929743  0.84147098  0.90929743] \n",
      "torch:\n",
      " tensor([-0.8415, -0.9093,  0.8415,  0.9093])\n"
     ]
    }
   ],
   "source": [
    "# sin\n",
    "print(\n",
    "    '\\nSin function-',\n",
    "    '\\nnumpy:\\n', np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]\n",
    "    '\\ntorch:\\n', torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.1192, 0.7311, 0.8808])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3679, 0.1353, 2.7183, 7.3891])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mean \n",
      "numpy:  0.0 \n",
      "torch:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "print(\n",
    "    '\\nmean',\n",
    "    '\\nnumpy: ', np.mean(data),         # 0.0\n",
    "    '\\ntorch: ', torch.mean(tensor)     # 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matrix multiplication (matmul) \n",
      "numpy:  [[ 7 10]\n",
      " [15 22]] \n",
      "torch:  tensor([[ 7., 10.],\n",
      "        [15., 22.]])\n"
     ]
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "data = [[1,2], [3,4]]\n",
    "tensor = torch.FloatTensor(data)  # 32-bit floating point\n",
    "# correct method\n",
    "print(\n",
    "    '\\nmatrix multiplication (matmul)',\n",
    "    '\\nnumpy: ', np.matmul(data, data),     # [[7, 10], [15, 22]]\n",
    "    '\\ntorch: ', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "matrix multiplication (dot) \n",
      "numpy:  [[ 7 10]\n",
      " [15 22]] \n",
      "torch: \n"
     ]
    }
   ],
   "source": [
    "# incorrect method\n",
    "data = np.array(data)\n",
    "tensor = torch.Tensor(data)\n",
    "print(\n",
    "    '\\nmatrix multiplication (dot)',\n",
    "    '\\nnumpy: ', data.dot(data),        # [[7, 10], [15, 22]]\n",
    "    '\\ntorch: ', #tensor.dot(tensor)     # NOT WORKING! Beware that torch.dot does not broadcast, only works for 1-dimensional tensor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "torch.dot(tensor1, tensor2) â†’ float\n",
    "\n",
    "Computes the dot product (inner product) of two tensors. Both tensors are treated as 1-D vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7., 10.],\n",
       "        [15., 22.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.mm(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 9., 16.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch variables and autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.FloatTensor([[1,2],[3,4]])            # build a tensor\n",
    "variable = Variable(tensor, requires_grad=True)      # build a variable, usually for compute gradients\n",
    "\n",
    "print(tensor)       # [torch.FloatTensor of size 2x2]\n",
    "print(variable)     # [torch.FloatTensor of size 2x2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till now the tensor and variable seem the same.\n",
    "\n",
    "However, the variable is a part of the graph, it's a part of the auto-gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.5000)\n",
      "tensor(0.3466, grad_fn=<CosBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_out = torch.mean(tensor*tensor)       # x^2\n",
    "v_out = torch.cos(torch.mean(variable*variable))   # x^2\n",
    "print(t_out)\n",
    "print(v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_out.backward()    # backpropagation from v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ v_{out} = {{1} \\over {4}} sum(variable^2) $$\n",
    "\n",
    "the gradients w.r.t the variable, \n",
    "\n",
    "$$ {d(v_{out}) \\over d(variable)} = {{1} \\over {4}} 2 variable = {variable \\over 2}$$\n",
    "\n",
    "let's check the result pytorch calculated for us below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4690, -0.9380],\n",
       "        [-1.4070, -1.8760]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable # this is data in variable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable.data # this is data in tensor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable.data.numpy() # numpy format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did `.backward()` on `v_out` but `variable` has been assigned new values on it's `grad`.\n",
    "\n",
    "As this line \n",
    "```\n",
    "v_out = torch.mean(variable*variable)\n",
    "``` \n",
    "will make a new variable `v_out` and connect it with `variable` in computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v_out.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the function is defined and computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "a = F.relu(x*w + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, PyTorch will automatically build a computation graph in the background if variables have the parameter `requires_grad=True` set.\n",
    "- If new variables without that parameter set to True are used in a computation with a variable that has `requires_grad=True`, these new variables will also automatically have gradients set to true.\n",
    "- This simply means that gradients for these variables will be computed.\n",
    "- It is wasteful to set `requires_grad=True` if we don't need that variable's gradient.\n",
    "- For example, we usually don't need the gradients of the training inputs `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the derivative of a with respect to w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.]),)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(a, w, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `retain_graph=True` keeps the computation graph in memory.\n",
    "- This is useful for example purposes to reuse the `grad` function.\n",
    "- In practice, we usually free the computation graph in every round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that PyTorch functions are usually more efficient, but we could also implement our own ReLU function as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.]),)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3.])\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "def my_relu(z):\n",
    "    if z > 0.:\n",
    "        return z\n",
    "    else:\n",
    "        z[:] = 0.\n",
    "        return z\n",
    "\n",
    "a = my_relu(x*w + b)\n",
    "grad(a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though the derivative of ReLU is not defined at 0, PyTorch autograd will do something that is reasonable for practical purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.]),)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-1.])\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "def my_relu(z):\n",
    "    if z > 0.:\n",
    "        return z\n",
    "    else:\n",
    "        z[:] = 0.\n",
    "        return z\n",
    "\n",
    "a = F.relu(x*w + b)\n",
    "grad(a, w, retain_graph=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Gradients in PyTorch via autograd's `grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, there are multiple ways to compute partial derivatives or gradients.\n",
    "- The most straightforward way to compute partial derivatives is using autograd's `grad` function.\n",
    "- By default, the `retain_graph` parameter of the `grad` function is set to `False`, which will free the graph after computing the partial derivative.\n",
    "- To obtain multiple partial derivatives, set `retain_graph=True`.\n",
    "- Note that setting `retain_graph=True` is inefficient as it requires multiple passes over the graph, recalculating intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_a_x: (tensor([2.]),)\n",
      "d_a_w: (tensor([3.]),)\n",
      "d_a_b: (tensor([1.]),)\n",
      "d_a_u: (tensor([1.]),)\n",
      "d_a_v: (tensor([1.]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x * w\n",
    "v = u + b\n",
    "a = F.relu(v)\n",
    "\n",
    "d_a_b = grad(a, b, retain_graph=True)\n",
    "d_a_u = grad(a, u, retain_graph=True)\n",
    "d_a_v = grad(a, v, retain_graph=True)\n",
    "d_a_w = grad(a, w, retain_graph=True)\n",
    "d_a_x = grad(a, x)\n",
    "    \n",
    "\n",
    "for name, dvar in zip(\"xwbuv\", (d_a_x, d_a_w, d_a_b, d_a_u, d_a_v)):\n",
    "    print('d_a_%s:' % name, dvar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A more efficient way is by passing a tuple to the `grad` function so that it can reuse intermediate results and only require one pass over the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_a_x: tensor([2.])\n",
      "d_a_w: tensor([3.])\n",
      "d_a_b: tensor([1.])\n",
      "d_a_u: tensor([1.])\n",
      "d_a_v: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x * w\n",
    "v = u + b\n",
    "a = F.relu(v)\n",
    "\n",
    "partial_derivatives = grad(a, (x, w, b, u, v))\n",
    "\n",
    "for name, grad in zip(\"xwbuv\", (partial_derivatives)):\n",
    "    print('d_a_%s:' % name, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Gradients in PyTorch via `retain_grad`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, we most often use the `backward()` method on an output variable to compute its partial derivative (or gradient) with respect to its inputs (typically, the weights and bias units of a neural network).\n",
    "- By default, PyTorch only stores the gradients of the leaf variables (e.g., the weights and biases) via their `grad` attribute to save memory.\n",
    "- If we are interested in the intermediate results in a computational graph, we can use the `retain_grad` method to store gradients of non-leaf variables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_a_x: tensor([2.])\n",
      "d_a_w: tensor([3.])\n",
      "d_a_b: tensor([1.])\n",
      "d_a_u: tensor([1.])\n",
      "d_a_v: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x * w\n",
    "v = u + b\n",
    "a = F.relu(v)\n",
    "\n",
    "u.retain_grad()\n",
    "v.retain_grad()\n",
    "\n",
    "a.backward()\n",
    "\n",
    "for name, var in zip(\"xwbuv\", (x, w, b, u, v)):\n",
    "    print('d_a_%s:' % name, var.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Gradients in PyTorch Using Hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can also use something called hooks to obtain intermediate gradients, although not a recommended approach as a first choice.\n",
    "- A hook is function that will be called every time a gradient with respect to the variable is computed. \n",
    "    (http://pytorch.org/docs/master/autograd.html#torch.autograd.Variable.register_hook)\n",
    "- We can use these hooks in a combination with a little helper function, `save_grad`, and a `hook` closure writing the partial derivatives or gradients to a global variable `grads`.\n",
    "- So, if we invoke the `backward` method on the output node `a`, all the intermediate results will be collected in `grads`, as illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "grads = {}\n",
    "def save_grad(name):\n",
    "    def hook(g):\n",
    "        grads[name] = g\n",
    "    return hook\n",
    "\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "w = torch.tensor([2.], requires_grad=True)\n",
    "b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "u = x * w\n",
    "v = u + b\n",
    "\n",
    "x.register_hook(save_grad('d_a_x'))\n",
    "w.register_hook(save_grad('d_a_w'))\n",
    "b.register_hook(save_grad('d_a_b'))\n",
    "u.register_hook(save_grad('d_a_u'))\n",
    "v.register_hook(save_grad('d_a_v'))\n",
    "\n",
    "a = F.relu(v)\n",
    "\n",
    "a.backward()\n",
    "\n",
    "#grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about sencond order derivative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y  = torch.tensor([4.])\n",
    "\n",
    "f = x**2 * y + y\n",
    "\n",
    "df_dx = grad(f, x) # 2xy = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([8.]),)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y  = torch.tensor([4.], requires_grad=True)\n",
    "\n",
    "f = x**2 * y + y\n",
    "\n",
    "df_dx = grad(f, x, create_graph=True) # first order deriv\n",
    "grad(df_dx, x) # 2y = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `retain_graph`: is meant for preserving the original graph for multiple backward passes with the same variables.\n",
    "- `create_graph`: is meant for constructing a new graph for the gradients, enabling the computation of higher-order derivatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor: tensor([1., 2., 3.], requires_grad=True)\n",
      "Output tensor: tensor([ 6., 12., 20.], grad_fn=<AddBackward0>)\n",
      "Elementwise gradient: (tensor([5., 7., 9.]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import grad\n",
    "import numpy as np\n",
    "\n",
    "# Define a vector function\n",
    "def vector_function(x):\n",
    "    return x**2 + 3*x + 2\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Compute the elementwise gradient\n",
    "y = vector_function(x)\n",
    "elementwise_grad = grad(y, x, grad_outputs=torch.ones_like(y))\n",
    "\n",
    "print(\"Input tensor:\", x)\n",
    "print(\"Output tensor:\", y)\n",
    "print(\"Elementwise gradient:\", elementwise_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
